{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADVANCED TEXT MINING\n",
    "- 본 자료는 텍스트 마이닝을 활용한 연구 및 강의를 위한 목적으로 제작되었습니다.\n",
    "- 본 자료를 강의 목적으로 활용하고자 하시는 경우 꼭 아래 메일주소로 연락주세요.\n",
    "- 본 자료에 대한 허가되지 않은 배포를 금지합니다.\n",
    "- 강의, 저작권, 출판, 특허, 공동저자에 관련해서는 문의 바랍니다.\n",
    "- **Contact : ADMIN(admin@teanaps.com)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEEK 03-2. 정적 페이지 수집하기: 실시간 검색어, 영화댓글\n",
    "- Python을 활용해 단순한 웹페이지에서 데이터를 크롤링하는 방법에 대해 다룹니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **\\*\\*\\* 주의사항 \\*\\*\\***  \n",
    "본 자료에서 설명하는 웹크롤링하는 방법은 해당 기법에 대한 이해를 돕고자하는 교육의 목적으로 사용되었으며,  \n",
    "이를 활용한 대량의 무단 크롤링은 범죄에 해당할 수 있음을 알려드립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 네이버 실시간 검색어 가져오기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ####  정적 페이지 크롤링 과정\n",
    "> 1. 진짜 URL 찾기  \n",
    "2. requests 패키지를 활용해 URL 호출하기\n",
    "3. BeautifulSoup과 HtmlParser 패키지로 HTML 태그 불러오기\n",
    "4. 불러온 페이지에 원하는 내용이 포함되어있는지 PRINT 문으로 확인하기 - 포함되어 있지 않으면 1번부터 다시시작!\n",
    "5. Chrome 브라우저 [메뉴] > [도구 더보기] > [개발자도구] 메뉴를 통해 원하는 내용이 포함된 태그 확인 후 호출\n",
    "6. 가져온 데이터 정리해서 파일에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 코드를 통해 웹페이지에 정보를 요청하기 위해 BeautifulSoup, urllib 패키지를 import 합니다.\n",
    "from bs4 import BeautifulSoup \n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 진짜 URL 찾기\n",
    "# 네이버 실시간 검색어는 네이버 데이터랩 급상승검색어 페이지에 존재합니다.\n",
    "URL = \"https://datalab.naver.com/keyword/realtimeList.naver?\"\n",
    "URL += \"age=10s\"                           # 연령\n",
    "URL += \"&datetime=2020-02-24T14%3A00%3A00\" # 집계주기\n",
    "URL += \"&entertainment=-2\"                 # 엔터\n",
    "URL += \"&groupingLevel=0\"                  # 이슈별 묶어보기\n",
    "URL += \"&marketing=-2\"                     # 이벤트/할인\n",
    "URL += \"&news=-2\"                          # 시사\n",
    "URL += \"&sports=-2\"                        # 스포츠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. requests 패키지를 활용해 URL 호출하기\n",
    "# 네이버 메인페이지 URL을 호출합니다.\n",
    "\n",
    "#response = requests.get(URL)\n",
    "\n",
    "headers = {\n",
    "    \"Referer\": \"https://datalab.naver.com/keyword/realtimeList.naver?age=20s&datetime=2020-09-19T14%3A00%3A00\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36\",\n",
    "}\n",
    "response = requests.get(URL, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. BeautifulSoup과 HtmlParser 패키지로 HTML 태그 불러오기\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 불러온 페이지에 원하는 내용이 포함되어있는지 PRINT 문으로 확인하기\n",
    "# PRINT 문을 활용해 불러온 웹페이지 내용을 출력합니다.\n",
    "\n",
    "#print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ctrl + F 단축키를 통해 현재 네이버 실시간 검색어 중 하나를 검색해 출력한 웹페이지 내용에 존재하는지 확인합니다.\n",
    "# 아래 예시는 \"런닝맨\"이 실시간 검색어 19위에 있음을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~html\n",
    "<li class=\"ah_item\" data-order=\"19\">\n",
    "  <a class=\"ah_a\" data-clk=\"lve.keyword\" data-ssl=\"https://search.naver.com/search.naver?where=nexearch&amp;query=%EB%9F%B0%EB%8B%9D%EB%A7%A8&amp;sm=top_lve&amp;ie=utf8\" href=\"http://search.naver.com/search.naver?where=nexearch&amp;query=%EB%9F%B0%EB%8B%9D%EB%A7%A8&amp;sm=top_lve&amp;ie=utf8\">\n",
    "    <span class=\"ah_r\">19</span>\n",
    "    <span class=\"ah_k\">런닝맨</span>\n",
    "  </a>\n",
    "  <a class=\"ah_da\" data-clk=\"lve.kwdhistory\" href=\"http://datalab.naver.com/keyword/realtimeDetail.naver?datetime=2019-06-03T04:45:00&amp;query=%EB%9F%B0%EB%8B%9D%EB%A7%A8&amp;where=main\">\n",
    "    <span class=\"blind\">데이터랩 그래프 보기</span>\n",
    "    <span class=\"ah_ico_datagraph\"></span>\n",
    "  </a>\n",
    "</li>\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Chrome 브라우저 [메뉴] > [도구 더보기] > [개발자도구] 메뉴를 통해 원하는 내용이 포함된 태그 확인 후 호출\n",
    "# 위 웹페이지 예시에서 \"런닝맨\" 텍스트는 아래와 같이 <span> 태그로 둘러쌓여 있습니다.\n",
    "# <span> 태그의 class 속성을 활용해 해당 태그만 가져올 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~html\n",
    "<span class=\"ah_k\">런닝맨</span>\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"item_title\">코로나나우</span>\n"
     ]
    }
   ],
   "source": [
    "# 5-1) 태그이름과 class 속성으로 해당 태그를 가져옵니다.\n",
    "tag_name = \"span\"\n",
    "class_name = \"item_title\"\n",
    "keyword = soup.find(tag_name, {'class': class_name,})\n",
    "print(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-2) 동일한 태그이름과 class 속성을 가진 태그를 모두 가져옵니다.\n",
    "# find 함수 대신 findAll 함수를 활용합니다.\n",
    "tag_name = \"span\"\n",
    "class_name = \"item_title\"\n",
    "keyword_list = soup.findAll(tag_name, {'class': class_name,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코로나나우\n",
      "단마토\n",
      "이탈리아 코로나\n",
      "이탈리아\n",
      "한국153\n",
      "다날\n",
      "코로나 학원 휴원\n",
      "안동시청\n",
      "순천 코로나\n",
      "코로나 대구 봉쇄\n",
      "황의조\n",
      "부산 강서구 코로나\n",
      "명성교회\n",
      "7번째 사망자\n",
      "코로나 현황\n",
      "순천\n",
      "하윤수\n",
      "대구 코로나 총괄팀장\n",
      "아시아드요양병원\n",
      "코호트\n"
     ]
    }
   ],
   "source": [
    "for keyword in keyword_list[:20]:\n",
    "    print(keyword.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코로나나우\n",
      "단마토\n",
      "이탈리아 코로나\n",
      "이탈리아\n",
      "한국153\n",
      "다날\n",
      "코로나 학원 휴원\n",
      "안동시청\n",
      "순천 코로나\n",
      "코로나 대구 봉쇄\n",
      "황의조\n",
      "부산 강서구 코로나\n",
      "명성교회\n",
      "7번째 사망자\n",
      "코로나 현황\n",
      "순천\n",
      "하윤수\n",
      "대구 코로나 총괄팀장\n",
      "아시아드요양병원\n",
      "코호트\n"
     ]
    }
   ],
   "source": [
    "# 태그가 저장된 변수에 get_text() 함수를 활용하면 태그로 둘러쌓인 텍스트만 가져올 수 있습니다.\n",
    "for keyword in keyword_list[:20]:\n",
    "    print(keyword.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 가져온 데이터 정리해서 파일에 저장하기\n",
    "# 새로운 파일을 생성해서 실시간 검색어를 저장합니다.\n",
    "f = open(\"naver_search.txt\", \"w\", encoding=\"utf-8\")\n",
    "for keyword in keyword_list[:20]:\n",
    "    text = keyword.get_text()\n",
    "    f.write(text + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코로나나우\n",
      "단마토\n",
      "이탈리아 코로나\n",
      "이탈리아\n",
      "한국153\n",
      "다날\n",
      "코로나 학원 휴원\n",
      "안동시청\n",
      "순천 코로나\n",
      "코로나 대구 봉쇄\n",
      "황의조\n",
      "부산 강서구 코로나\n",
      "명성교회\n",
      "7번째 사망자\n",
      "코로나 현황\n",
      "순천\n",
      "하윤수\n",
      "대구 코로나 총괄팀장\n",
      "아시아드요양병원\n",
      "코호트\n"
     ]
    }
   ],
   "source": [
    "# 파일에 저장된 실시간 검색어를 불러옵니다.\n",
    "f = open(\"naver_search.txt\", encoding=\"utf-8\")\n",
    "for keyword in f:\n",
    "    print(keyword.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 네이버 영화댓글 가져오기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 코드를 통해 웹페이지에 정보를 요청하기 위해 BeautifulSoup, urllib 패키지를 import 합니다.\n",
    "from bs4 import BeautifulSoup \n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 진짜 URL 찾기\n",
    "# 네이버 영화댓글이 포함된 페이지의 URL을 가져옵니다.\n",
    "URL = \"https://movie.naver.com/movie/bi/mi/point.nhn?code=190010#tab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. urllib 패키지를 활용해 URL 호출하기\n",
    "# 네이버 영화댓글 페이지 URL을 호출합니다.\n",
    "response = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. BeautifulSoup과 HtmlParser 패키지로 URL 호출 응답 불러오기\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 불러온 페이지에 원하는 내용이 포함되어있는지 PRINT 문으로 확인하기\n",
    "# PRINT 문을 활용해 불러온 웹페이지 내용을 출력합니다.\n",
    "\n",
    "#print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ctrl + F 단축키를 통해 댓글 중 하나의 일부 또는 전체를 검색해 출력한 웹페이지 내용에 존재하는지 확인합니다.\n",
    "# 해당 웹페이지 내용에 댓글이 존재하지 않습니다.\n",
    "# 다시 \"1.진짜 URL 찾기\" 과정으로 돌아갑니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 진짜 URL 찾기\n",
    "# 네이버 영화댓글이 포함된 페이지의 URL을 가져옵니다.\n",
    "URL = \"https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=167491&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page=2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. urllib 패키지를 활용해 URL 호출하기\n",
    "# 네이버 영화댓글 페이지 URL을 호출합니다.\n",
    "response = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. BeautifulSoup과 HtmlParser 패키지로 URL 호출 응답 불러오기\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 불러온 페이지에 원하는 내용이 포함되어있는지 PRINT 문으로 확인하기\n",
    "# PRINT 문을 활용해 불러온 웹페이지 내용을 출력합니다.\n",
    "\n",
    "#print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~html\n",
    "<div class=\"score_reple\">\n",
    "  <p>마블 3000만큼 사랑합니다  </p>\n",
    "  <dl>\n",
    "    <dt>\n",
    "      <em>\n",
    "        <a href=\"#\" onclick=\"javascript:showPointListByNid(15535812, 'after');parent.clickcr(this, 'ara.uid', '', '', event); return false;\" target=\"_top\">\n",
    "          <span>전은호(eunh****)</span>\n",
    "        </a>\n",
    "      </em>\n",
    "      <em>2019.04.24 10:31</em>\n",
    "    </dt>\n",
    "    <dd>\n",
    "      <a class=\"go_report2\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.report', '', '', event); common.report('false','eunh****', 'uv551WV2CAQ0fZUWTaiFpIb20w73doxxmi6uj3BLyPs=', '마블 3000만큼 사랑합니다 ', '15535812', 'point_after', false);return false;\"><em>신고</em></a>\n",
    "    </dd>\n",
    "  </dl>\n",
    "</div>\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Chrome 브라우저 [메뉴] > [도구 더보기] > [개발자도구] 메뉴를 통해 원하는 내용이 포함된 태그 확인 후 호출\n",
    "# 위 웹페이지 예시에서 \"마블 3000만큼 사랑합니다\" 텍스트는 <p> 태그로 둘러쌓여 있습니다.\n",
    "# <p> 태그로도 해당 부분을 가져올 수 있지만 정확도를 높이기 위해 그 상위태그인 <div> 태그를 활용합니다.\n",
    "# <div> 태그의 class 속성을 활용해 해당 태그만 가져올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-2) 동일한 태그이름과 class 속성을 가진 태그를 모두 가져옵니다.\n",
    "# 한 페이지에 댓글이 여러개 존재하므로 find 함수 대신 findAll 함수를 활용합니다.\n",
    "tag_name = \"div\"\n",
    "class_name = \"score_reple\"\n",
    "comment_list = soup.findAll(tag_name, {'class': class_name,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"score_reple\">\n",
      "<p>\n",
      "<!-- 스포일러 컨텐츠로 처리되는지 여부 -->\n",
      "<span id=\"_filtered_ment_0\">\r\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t디즈니..넘 실망....어떻게 그럴수있니 \r\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n",
      "</p>\n",
      "<dl>\n",
      "<dt>\n",
      "<em>\n",
      "<a href=\"#\" onclick=\"javascript:showPointListByNid(17143657, 'after');parent.clickcr(this, 'ara.uid', '', '', event); return false;\" target=\"_top\">\n",
      "<span>토이스토리(gpff****)</span>\n",
      "</a>\n",
      "</em>\n",
      "<em>2020.09.17 13:15</em>\n",
      "</dt>\n",
      "<dd>\n",
      "<a class=\"go_report2\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.report', '', '', event); common.report('false','gpff****', '99yIjhQJahRlX1vJXOfBavJWWwtkoraRaRLOFw5sD0M=', '디즈니..넘 실망....어떻게 그럴수있니 ', '17143657', 'point_after', false);return false;\"><em>신고</em></a>\n",
      "</dd>\n",
      "</dl>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "# 웹페이지에서 댓글을 태그로 특정할때 <p> 태그가 아닌 상위태그 <div>를 활용했기 때문에 <div> 태그 하위태그가 모두 출현합니다.\n",
    "for comment in comment_list:\n",
    "    print(comment)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>\n",
      "<!-- 스포일러 컨텐츠로 처리되는지 여부 -->\n",
      "<span id=\"_filtered_ment_0\">\r\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t디즈니..넘 실망....어떻게 그럴수있니 \r\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n",
      "</p>\n"
     ]
    }
   ],
   "source": [
    "# find 함수를 한번 더 활용하여 <div> 태그 하위태그 중 <p> 태그만 골라 가져옵니다.\n",
    "# class 속성이 없는 경우 입력하지 않아도 됩니다.\n",
    "for comment in comment_list:\n",
    "    print(comment.find(\"p\"))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디즈니..넘 실망....어떻게 그럴수있니\n",
      "기대도 안했는데 기대이하였음 디즈니에 개실망\n",
      "영화에 짜장이 묻으면 이 꼴 나는구나 ㅋ\n",
      "내 어린시절 뮬란을 이렇게 망치지마\n",
      "중국이 싫다고 1점 주지는 맙시다. 저는 이 영화가 싫어서 1점 줍니다.\n",
      "돈으로 헐리우드를 샀지만 관객마음은 못사지\n",
      "개봉하기전에 이미 봤다...ㅋㅋㅋㅋ견자단하고 이연걸 아깝다... 여기 왜 출연했어..\n",
      "이번만큼은 동의 합니다. 찌꺼기라는 평론가의 평점을..\n",
      "이번 영화는 그저 유역비를 보고싶어하는 이들을 위한 팬심 영화인듯 이거 보지 마시고 예고편이 더 재밌어요\n",
      "21세기 희대의 망작. 디즈니 주주로써 화가 치밀어 오른다\n"
     ]
    }
   ],
   "source": [
    "# findAll 함수를 한번 더 활용하여 <p> 태그 하위태그 중 <span> 태그만 골라 가져옵니다.\n",
    "for comment in comment_list:\n",
    "    #print(comment.find(\"p\").findAll(\"span\"))\n",
    "    span_tag_list = comment.find(\"p\").findAll(\"span\")#.findALL(\"span\")\n",
    "    if span_tag_list[0].text == \"관람객\":\n",
    "        print(span_tag_list[1].text.strip())\n",
    "    else:\n",
    "        print(span_tag_list[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 가져온 데이터 정리해서 파일에 저장하기\n",
    "# 새로운 파일을 생성해서 영화댓글을 저장합니다.\n",
    "f = open(\"movie_comment.txt\", \"w\", encoding=\"utf-8\")\n",
    "for comment in comment_list:\n",
    "    #print(comment.find(\"p\").findAll(\"span\"))\n",
    "    span_tag_list = comment.find(\"p\").findAll(\"span\")#.findALL(\"span\")\n",
    "    if span_tag_list[0].text == \"관람객\":\n",
    "        f.write(span_tag_list[1].text.strip() + '\\n')\n",
    "    else:\n",
    "        f.write(span_tag_list[0].text.strip() + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디즈니..넘 실망....어떻게 그럴수있니\n",
      "기대도 안했는데 기대이하였음 디즈니에 개실망\n",
      "영화에 짜장이 묻으면 이 꼴 나는구나 ㅋ\n",
      "내 어린시절 뮬란을 이렇게 망치지마\n",
      "중국이 싫다고 1점 주지는 맙시다. 저는 이 영화가 싫어서 1점 줍니다.\n",
      "돈으로 헐리우드를 샀지만 관객마음은 못사지\n",
      "개봉하기전에 이미 봤다...ㅋㅋㅋㅋ견자단하고 이연걸 아깝다... 여기 왜 출연했어..\n",
      "이번만큼은 동의 합니다. 찌꺼기라는 평론가의 평점을..\n",
      "이번 영화는 그저 유역비를 보고싶어하는 이들을 위한 팬심 영화인듯 이거 보지 마시고 예고편이 더 재밌어요\n",
      "21세기 희대의 망작. 디즈니 주주로써 화가 치밀어 오른다\n"
     ]
    }
   ],
   "source": [
    "# 파일에 저장된 실시간 검색어를 불러옵니다.\n",
    "f = open(\"movie_comment.txt\", encoding=\"utf-8\")\n",
    "for comment in f:\n",
    "    print(comment.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Assignment 02. 네이버 영화댓글을 대량으로 가져오기\n",
    "> - 본 자료를 응용하여 네이버 영화댓글을 부가정보와 함께 대량으로 가져오는 코드를 작성합니다.\n",
    "> - 영화댓글 페이지의 100페이지 분량의 댓글을 수집합니다.\n",
    "> - 영화댓글 텍스트와 함께, 작성일자와 평점을 같이 가져옵니다.\n",
    "> - 영화댓글, 작성일자, 평점을 탭(\\t) 단위로 구분하여 파일에 저장합니다.  \n",
    "\n",
    "> HINT. 영화댓글 URL 맨 뒤의 숫자는 페이지 번호를 의미합니다. (\"&page=1\")\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "\n",
    "PAGE_LIMIT = 257\n",
    "SAVE_FILE_PATH = \"movie_comment.txt\"\n",
    "\n",
    "f = open(SAVE_FILE_PATH, \"w\", encoding=\"utf-8\")\n",
    "for page in range(1, PAGE_LIMIT+1):\n",
    "    print(page, end=\"\\r\")\n",
    "    URL = \"https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=193194&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page=\"\n",
    "    URL = URL + str(page)\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    comment_list = soup.findAll(\"div\", {'class': \"score_reple\",})\n",
    "    for comment in comment_list:\n",
    "        span_tag_list = comment.find(\"p\").findAll(\"span\")\n",
    "        if span_tag_list[0].text == \"관람객\":\n",
    "            #print(span_tag_list[1].text.strip())\n",
    "            f.write(span_tag_list[1].text.strip() + \"\\n\")\n",
    "        else:\n",
    "            #print(span_tag_list[0].text.strip())\n",
    "            f.write(span_tag_list[0].text.strip() + \"\\n\")\n",
    "    f.flush()\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
